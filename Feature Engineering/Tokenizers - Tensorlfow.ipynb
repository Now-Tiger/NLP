{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c87869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ae5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I love my dog\", \n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27cd14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog', 'I love my cat', 'You love my dog!']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77a12a",
   "metadata": {},
   "source": [
    "## Tokenizer üé´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2038edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n",
      "\n",
      "OrderedDict([('i', 2), ('love', 3), ('my', 3), ('dog', 2), ('cat', 1), ('you', 1)])\n",
      "\n",
      "defaultdict(<class 'int'>, {'my': 3, 'i': 2, 'love': 3, 'dog': 2, 'cat': 1, 'you': 1})\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# - word_index assigns a unique index to each word present in the text.\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# - The word_count shows the number of times words occur in the text corpus\n",
    "word_counts = tokenizer.word_counts\n",
    "\n",
    "# - The word_doc tells in how many documents each of the words appear\n",
    "word_in_doc = tokenizer.word_docs\n",
    "\n",
    "# - Number of documents/texts passed to the keras tokenizer class.\n",
    "documents = tokenizer.document_count\n",
    "\n",
    "print(word_index, word_counts, word_in_doc, documents, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15115d",
   "metadata": {},
   "source": [
    "## texts_to_sequences üìö\n",
    "\n",
    "- method helps in converting tokens of text corpus into a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c57736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog', 'I love my cat', 'You love my dog!', 'Do you think my dog is amazing?']\n"
     ]
    }
   ],
   "source": [
    "sentences.append(\"Do you think my dog is amazing?\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb879e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293a677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index: {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "token sequences: [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "toks = tf.keras.preprocessing.text.Tokenizer(num_words = 100)\n",
    "toks.fit_on_texts(sentences)\n",
    "\n",
    "# - Generate word index dictionary\n",
    "word_idx = toks.word_index\n",
    "\n",
    "# - List of token sequences\n",
    "sequences = toks.texts_to_sequences(sentences)\n",
    "\n",
    "\n",
    "print(f\"word index: {word_idx}\\n\"\n",
    "      f\"token sequences: {sequences}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206c54e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "# - test the tokenizer \n",
    "test_data = [\n",
    "    \"i really love my dog\", \n",
    "    \"my dog loves my manatee\",\n",
    "]\n",
    "\n",
    "test_seq = toks.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b386839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
     ]
    }
   ],
   "source": [
    "print(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e17acbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq[1] # <= gave 'my dog my'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06fcff8",
   "metadata": {},
   "source": [
    "- __Note:__\n",
    "- To takle this problem we need more training data so the tokeizer can learn to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc0963",
   "metadata": {},
   "source": [
    "## Try to resolve above issue üß™\n",
    "\n",
    "- Firs solution is to gather much more data.\n",
    "- Second solution is to put a special value/character to those words which are not seen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99378062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog', 'I love my cat', 'You love my dog!', 'Do you think my dog is amazing?']\n"
     ]
    }
   ],
   "source": [
    "# - Lets look at out data once:\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b1282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i really love my dog', 'my dog loves my manatee']\n",
      "\n",
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "# - Apply second solution\n",
    "new_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 100, oov_token = \"<00V>\")\n",
    "new_tokenizer.fit_on_texts(sentences)\n",
    "word_index = new_tokenizer.word_index\n",
    "\n",
    "sequences_new = new_tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "test_seq_new = new_tokenizer.texts_to_sequences(test_data)\n",
    "print(test_data, test_seq_new, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1d474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<00V>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "# - we can also call word_index dictionary as a lookup dictionary.\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96152aa",
   "metadata": {},
   "source": [
    "- __still not syntactically correct!__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcf8b5",
   "metadata": {},
   "source": [
    "## padding üìè\n",
    "\n",
    "- Further when we feed data to the neural network model(training) we need data in uniform size.\n",
    "- Therefor __padding__ is used to make uniform sequence of data. \n",
    "- List that has maximum length will get the size to all of lists of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6329f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog', 'I love my cat', 'You love my dog!', 'Do you think my dog is amazing?']\n",
      "\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(sentences, sequences_new, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bad8cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "pad_sequences = tf.keras.utils.pad_sequences(sequences_new,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee7c3348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog', 'I love my cat', 'You love my dog!', 'Do you think my dog is amazing?']\n",
      "\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n",
      "\n",
      "{'<00V>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "print(sentences, sequences_new, pad_sequences, word_index, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f64380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on going..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
