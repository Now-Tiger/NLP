{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9237d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import shutil\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "LOG_DIR = '../logs/'\n",
    "sns.set_style('whitegrid')\n",
    "filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='../logs/')\n",
    "jtplot.style(context='talk', theme='chesterish', grid=True, ticks=True, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237454ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../Data/Imdb/\"\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0f014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The train directory also has additional folders which should be removed before creating training dataset.\n",
    "# remove_dir = os.path.join(train_dir, 'unsup')\n",
    "# shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562fab4",
   "metadata": {},
   "source": [
    "## Create Datasets:\n",
    "\n",
    "- use train directory to create train dataset and a validation dataset with split of 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbf8ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "SEED = 123\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../Data/Imdb/train/', batch_size=BATCH_SIZE, \n",
    "    validation_split=.2, subset='training', seed=SEED\n",
    ")\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../Data/Imdb/train', batch_size=BATCH_SIZE, \n",
    "    validation_split=.2, subset='validation', seed=SEED\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c82698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: b\"Oh My God! Please, for the love of all that is holy, Do Not Watch This Movie! It it 82 minutes of my life I will never get back. Sure, I could have stopped watching half way through. But I thought it might get better. It Didn't. Anyone who actually enjoyed this movie is one seriously sick and twisted individual. No wonder us Australians/New Zealanders have a terrible reputation when it comes to making movies. Everything about this movie is horrible, from the acting to the editing. I don't even normally write reviews on here, but in this case I'll make an exception. I only wish someone had of warned me before I hired this catastrophe\"\n",
      "1: b'This movie is SOOOO funny!!! The acting is WONDERFUL, the Ramones are sexy, the jokes are subtle, and the plot is just what every high schooler dreams of doing to his/her school. I absolutely loved the soundtrack as well as the carefully placed cynicism. If you like monty python, You will love this film. This movie is a tad bit \"grease\"esk (without all the annoying songs). The songs that are sung are likable; you might even find yourself singing these songs once the movie is through. This musical ranks number two in musicals to me (second next to the blues brothers). But please, do not think of it as a musical per say; seeing as how the songs are so likable, it is hard to tell a carefully choreographed scene is taking place. I think of this movie as more of a comedy with undertones of romance. You will be reminded of what it was like to be a rebellious teenager; needless to say, you will be reminiscing of your old high school days after seeing this film. Highly recommended for both the family (since it is a very youthful but also for adults since there are many jokes that are funnier with age and experience.'\n"
     ]
    }
   ],
   "source": [
    "# -- Checking first five movie reviews from the datasets(tesnroflow dataset object)\n",
    "for text, label in train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        print(label[i].numpy(), text.numpy()[i], sep=\": \", end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bcc33d",
   "metadata": {},
   "source": [
    "## Configure the dataset for performance:\n",
    "\n",
    "- `.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.<br><br>\n",
    "\n",
    "- `.prefetch()` overlaps data preprocessing and model execution while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2715c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7369312",
   "metadata": {},
   "source": [
    "## Using the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75ea694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layers = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057277a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.03979614  0.00108985 -0.00313186  0.04942242 -0.0214565 ]\n",
      " [-0.01830754  0.04851105 -0.01170666 -0.02710489  0.02220693]\n",
      " [-0.01305484 -0.02601485 -0.02959191 -0.02536185 -0.04980961]\n",
      " [ 0.00181361 -0.03030678 -0.00508846 -0.01310108  0.03067455]], shape=(4, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# -- test out embedding layer \n",
    "# -- its a tensor of integers, of shape (samples, sequence_length)\n",
    "print(embedding_layers(tf.constant([1, 2, 3, 55])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ebd468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 8.9626312e-03  4.5869675e-02 -2.3183061e-02  4.0337298e-02\n",
      "   -4.2887103e-02]\n",
      "  [ 3.9796140e-02  1.0898486e-03 -3.1318553e-03  4.9422417e-02\n",
      "   -2.1456504e-02]\n",
      "  [-1.8307544e-02  4.8511054e-02 -1.1706661e-02 -2.7104890e-02\n",
      "    2.2206929e-02]]\n",
      "\n",
      " [[-1.3054837e-02 -2.6014853e-02 -2.9591907e-02 -2.5361849e-02\n",
      "   -4.9809612e-02]\n",
      "  [ 1.8116523e-02  2.8409135e-02 -3.0370235e-02  4.2130638e-02\n",
      "    4.8429970e-02]\n",
      "  [-2.8947508e-02 -3.7939988e-02 -7.5232238e-05  2.9212918e-02\n",
      "   -3.4238055e-02]]], shape=(2, 3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "res = embedding_layers(tf.constant([[0, 1, 2], [3, 4, 5]]))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9baf3",
   "metadata": {},
   "source": [
    "- This returned tensor has one more axis, than the input, the embedding vectors are alinged along the new last axis.\n",
    "- When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009e5db",
   "metadata": {},
   "source": [
    "## Text Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c78309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    \"\"\" custom standardization function to strip HTML break tags \"\"\"\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '', '')\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, '[%s]' % re.escape(string.punctuation), ''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4180e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- vocabulary size & number of words in a sequence\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "# -- using the text-vectorization layer to normalize, split, and map strings to integers.\n",
    "# -- layer uses custom standardization defined above.\n",
    "# -- set maximum sequence length as all samples are not of the same length.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization, \n",
    "    max_tokens=vocab_size, \n",
    "    output_mode='int', \n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "# -- make text only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36a26328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'the original animated dark knight returns in this ace adventure movie that rivals mask of phantasm in its coolness theres a lot of style and intelligence in mystery of the batwoman so much more than batman forever or batman and robinbr br theres a new crimefighter on the streets of gotham she dresses like a bat but shes not a grownup batgirl and batman is denying any affiliation with her meanwhile bruce wayne has to deal with the usual romances and detective work but the penguin bain and the local mob makes things little more complicatedbr br i didnt have high hopes for this un since being strongly let down but the weak batman sub zero robin isnt featured so much herebut i was delighted with the imaginative and exciting set pieces the clever plot and a cheeky sense of humor this is definitely a movie no fan of batman should be without keep your ears open for a really catchy song called betcha neva which is featured prominently throughoutbr br its a shame the dvd isnt so great dont get me wrong there are some great features the short chase me is awesome and a very cool dolby 51 soundtrack but the movie is presented in pan and scan batman mystery of the batwoman was drawn and shot in 1851 but this dvd is presented in 1331 an in comparison to the widescreen clips shown on the features there is picture cut off on both sides i find this extremely annoying considering mask of phantasm was presented in anamorphic widescreen warner have had to rerelease literally dozens of movies on dvd because people have complained about the lack of original aspect ratio available on some titles why they chose to make that same mistake here again is beyond mebr br i would give this dvd 55 but the lack of oar brings the overall score down to 45 its a shame because widescreen would have completed a great dvd package'\n",
      "Label: 1\n",
      "\n",
      "Vectorized review: (<tf.Tensor: shape=(1, 100), dtype=int64, numpy=\n",
      "array([[   2,  197, 1094,  456, 5529, 1741,    8,   11, 6586, 1209,   18,\n",
      "          12, 7669, 2273,    5, 6876,    8,   30,    1,  211,    4,  170,\n",
      "           5,  433,    3, 1635,    8,  774,    5,    2, 9024,   38,   73,\n",
      "          52,   72, 1389, 1425,   42, 1389,    3,    1,   13,  211,    4,\n",
      "         157,    1,   21,    2, 1879,    5,    1,   55, 5345,   39,    4,\n",
      "        3497,   19,  426,   22,    4,    1,    1,    3, 1389,    7, 8394,\n",
      "          98,    1,   16,   40, 2137, 1382, 2476,   44,    6,  838,   16,\n",
      "           2,  608, 6745,    3, 1249,  163,   19,    2,    1,    1,    3,\n",
      "           2,  680, 3055,  159,  182,  111,   52,    1,   13,   10,  153,\n",
      "          26]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "# -- creating a function to see the results of using this layer to preprocess some data.\n",
    "def vec_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "text_batch, label_batch = next(iter(train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "processed_review = custom_standardization(first_review)\n",
    "print(f'Review: {processed_review}\\n' \n",
    "      f'Label: {first_label}\\n\\n'\n",
    "      f'Vectorized review: {vec_text(processed_review, first_label)}'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47fcd068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'the original animated dark knight returns in this ace adventure movie that rivals mask of phantasm in its coolness theres a lot of style and intelligence in mystery of the batwoman so much more than batman forever or batman and robinbr br theres a new crimefighter on the streets of gotham she dresses like a bat but shes not a grownup batgirl and batman is denying any affiliation with her meanwhile bruce wayne has to deal with the usual romances and detective work but the penguin bain and the local mob makes things little more complicatedbr br i didnt have high hopes for this un since being strongly let down but the weak batman sub zero robin isnt featured so much herebut i was delighted with the imaginative and exciting set pieces the clever plot and a cheeky sense of humor this is definitely a movie no fan of batman should be without keep your ears open for a really catchy song called betcha neva which is featured prominently throughoutbr br its a shame the dvd isnt so great dont get me wrong there are some great features the short chase me is awesome and a very cool dolby 51 soundtrack but the movie is presented in pan and scan batman mystery of the batwoman was drawn and shot in 1851 but this dvd is presented in 1331 an in comparison to the widescreen clips shown on the features there is picture cut off on both sides i find this extremely annoying considering mask of phantasm was presented in anamorphic widescreen warner have had to rerelease literally dozens of movies on dvd because people have complained about the lack of original aspect ratio available on some titles why they chose to make that same mistake here again is beyond mebr br i would give this dvd 55 but the lack of oar brings the overall score down to 45 its a shame because widescreen would have completed a great dvd package', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "standardized_ = custom_standardization(first_review.numpy())\n",
    "print(standardized_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c99fb",
   "metadata": {},
   "source": [
    "## Create a classification model: Continuous Bag of Words model.\n",
    "- Now vectorize_layer can be used as the first layer of your end-to-end classification model, feeding transformed strings into the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c07d909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer, \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, name='embedding'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f971f3",
   "metadata": {},
   "source": [
    "## Compile & Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eaf029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 11s 499ms/step - loss: 0.6922 - accuracy: 0.5028 - val_loss: 0.6909 - val_accuracy: 0.4886\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 9s 438ms/step - loss: 0.6886 - accuracy: 0.5028 - val_loss: 0.6863 - val_accuracy: 0.4886\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 9s 437ms/step - loss: 0.6825 - accuracy: 0.5028 - val_loss: 0.6793 - val_accuracy: 0.4886\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 9s 435ms/step - loss: 0.6732 - accuracy: 0.5028 - val_loss: 0.6686 - val_accuracy: 0.4886\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 9s 439ms/step - loss: 0.6596 - accuracy: 0.5028 - val_loss: 0.6539 - val_accuracy: 0.4886\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 9s 439ms/step - loss: 0.6413 - accuracy: 0.5028 - val_loss: 0.6348 - val_accuracy: 0.4886\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 9s 435ms/step - loss: 0.6182 - accuracy: 0.5036 - val_loss: 0.6120 - val_accuracy: 0.4958\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 9s 441ms/step - loss: 0.5912 - accuracy: 0.5602 - val_loss: 0.5866 - val_accuracy: 0.5848\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 9s 442ms/step - loss: 0.5616 - accuracy: 0.6439 - val_loss: 0.5604 - val_accuracy: 0.6376\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 9s 439ms/step - loss: 0.5311 - accuracy: 0.6995 - val_loss: 0.5347 - val_accuracy: 0.6796\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 9s 451ms/step - loss: 0.5013 - accuracy: 0.7404 - val_loss: 0.5107 - val_accuracy: 0.7136\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 9s 450ms/step - loss: 0.4731 - accuracy: 0.7663 - val_loss: 0.4893 - val_accuracy: 0.7342\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 9s 440ms/step - loss: 0.4473 - accuracy: 0.7893 - val_loss: 0.4706 - val_accuracy: 0.7516\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 9s 447ms/step - loss: 0.4239 - accuracy: 0.8069 - val_loss: 0.4546 - val_accuracy: 0.7588\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 9s 440ms/step - loss: 0.4028 - accuracy: 0.8199 - val_loss: 0.4410 - val_accuracy: 0.7686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x277974b6fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=15, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44d59c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Visualize the graph of performance\n",
    "# (env) d:drive>python -m tensorboard.main --logdir=logs/   <-- worked for me\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd156578",
   "metadata": {},
   "source": [
    "## Retrieve the trained word embeddings and save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39cd9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Write the weights to disk\n",
    "# 1. file of vectors containing the embedding : tsv- tab separated format\n",
    "# 2. file of metadata containing words.\n",
    "# -----------------------------------------------------------------------\n",
    "# weights = model.get_layer('embedding').get_weights()[0]\n",
    "# vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "# out_v = io.open(LOG_DIR, 'w', encoding='utf-8')\n",
    "# out_m = io.open(LOG_DIR, 'w', encoding='utf-8')\n",
    "\n",
    "# for idx, word in enumerate(vocab):\n",
    "#     if idx == 0:\n",
    "#         continue\n",
    "#     vec = weights[idx]\n",
    "#     out_v.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "#     out_m.write(word + \"\\n\")\n",
    "# out_v.close()\n",
    "# out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1274a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 336ms/step - loss: 0.4410 - accuracy: 0.7686\n",
      "Accuracy: 0.77 | Loss: 0.44\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(val_ds)\n",
    "print(f\"Accuracy: {accuracy:.2f} | Loss: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11999037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- fetch the test dataset and make predictions on that.\n",
    "# -- Improve the accuracy of the mode as the model is overfitting on the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
